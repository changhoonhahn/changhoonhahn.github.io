<!DOCTYPE HTML>
<html>
<head>
<meta charset="utf-8">
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<title>ChangHoon Hahn --- Research</title>
<meta id="viewport" name="viewport" content="width=500" />
<meta name="description" content="Chang is a postdoc at Berkeley Lab" />

<link rel="stylesheet" href="css/normalize.css" type="text/css" />
<link rel="stylesheet" href="css/base.css" type="text/css" />
<link rel="stylesheet" href="css/code.css" type="text/css" />

<link rel="icon" type="image/png" href="images/chicago.png" sizes="300x300"> 
<!--<link rel="icon" type="image/png" href="{{ SITEURL }}/theme/images/favicon.png" sizes="32x32">  -->
<style> 
    #research a{
        color:DarkBlue; 
    }
    #research figcaption{
        line-height:15px; 
    }
</style>
</head>
<body>

<div class="content">

    <header>
        <a href="index.html"><img src="images/chicago.png" alt="me"></a>
        <h1><a href="index.html">ChangHoon Hahn</a></h1>
        <ul>
            <li></li>
            <li><a href="research.html">Research</a></li>
            <li><a href="publications.html">Publications</a></li>
            <li><a href="cv/CV.pdf">CV</a></li>
            <li><a href="code.html">Code</a></li>
        </ul>
    </header>
    <section id="research"> 
        My research interests include  
        <a href="stats.html">astrostatistics and ML</a>, 
        <a href="galaxy.html">galaxy evolution</a>, and
        <a href="cosmo.html">cosmology</a>.
        <p>
        <p>
        <h2 id="stats"><u>Simulation-Based Inference</u></h2></br>
        Probabilistic inference is a pivotal step in any scientific experiment.
        It provides the statistical framework for comparing theoretical
        models that encapsulate our knowledge of physics processes to
        observations and derive constraints on the parameters of interest â€”
        <i>i.e.</i> physics. 
        In cosmology, for instance, we use inference to measure the contents of
        the Universe from 3D maps of galaxies.
        In galaxy evolution, inference allows us to measure the physical
        properties of galaxies from their spectra.
        <p>
        A new class of methods in simulation-based inference (SBI; also known as
        "likelihood-free inference") is transforming probabilistic inference. 
        SBI enables rigorous inference using only simulations that describe the
        observations and the physical phenomena of interest. 
        Cutting-edge SBI methods exploit neural density estimation and can
        efficiently infer high-dimensional posteriors, the probability
        distribution of parameters given observations. 
        SBI relaxes the assumptions in standard approaches for more accurate
        and rigorous inference. 
        It also enables us to leverage the predictive power of high-fidelity
        simulations to more tightly constrain physics.
        </p>
        <p>
        I am a leading expert on SBI. 
        I pioneered SBI in large-scale structure
        [<a href="https://arxiv.org/abs/1607.01782"><font color='DarkBlue'>1607.01782</font></a>]
        and <a href="galaxy.html">galaxy evolution studies</a>. 
        I have also developed new SBI methods [<a href="https://arxiv.org/abs/1803.06348"><font color='DarkBlue'>1803.06348</font></a>].
        Last semester (Fall 2021), I co-developed and instructed a graduate
        course on SBI at Princeton. 
        I also organized workshops on SBI at the 
        <a href="https://lfitaskforce.github.io/FlatironMeeting/">Flatiron Institute</a> 
        and at 
        <a href="https://bids.berkeley.edu/events/bay-area-likelihood-free-inference-meeting">Berkeley Institute for Data Science</a> 
        that brought together SBI experts from across disciplines to share the
        latest techniques, use cases and applications, and to discuss open
        challenges. 
        Much of my current research focuses on developing new SBI methods and
        exploiting them to maximize the scientific return of upcoming galaxy
        surveys, starting with DESI. 
        </p>

        <h2 id="stats"><u>ML for Big Data Astronomy</u></h2></br>
        With upcoming observations from <a href="cosmo.html">DESI</a>, PFS,
        JWST, Rubin, and Roman, astronomy is entering the era of Big Data.  
        ML techniques offer new ways to tackle the upcoming massive data sets.
        One key application of ML is for accelerating our models and
        simulations: <i>e.g.</i> in [<a href="https://arxiv.org/abs/1910.04255">1910.04255</a>]
        we used ML to accurately construct N-body simulations with massive
        neutrinos that originally takes 700 CPU hours in just 4 mins. 
        Similarly, in [<a href="https://arxiv.org/abs/1911.11778">1911.11778</a>] 
        we used neural emulators to make SED models >1000x times faster.
        </p>
        <p>
        ML also provides essential tools to construct scalable analyses through
        SBI.
        Certain SBI methods enable amortized inference by using neural density
        estimate of the posterior over the full conditional space of
        observations. 
        Once trained, they can be applied to data to infer the posterior from
        each observation in seconds, effectively eliminating the computational
        cost of inference. 
        Using SBI, we can deploy rigorous and sophisticated analyses to the
        billions of galaxies that will be observed by the next-generation
        experiments.
        For an example, check out my 
        <a href="atlas_poster.html">interactive slides</a> 
        from a recent 
        <a href="http://www.iastro.pt/research/conferences/atlas21/"> conference</a>. 
        </p> 
        <p>
        I am also developing unsupervised deep generative models for generating
        realistic simulations. 
        In particular, I am building generative models that incorporate our
        knowledge of physics. 
        For example, most of the light in galaxy spectra comes from stellar
        populations that can be modeled using stellar population synthesis
        (SPS).
        I am currently developing a generative model for galaxy spectra that
        incorporates SPS within its architecture. 
        It also includes physical transformations such as redshifting within
        its causal structure.
        This physics-driven generative model produces galaxy spectra that are
        more realistic and orders of magnitude faster than current models.
        More broadly, these ML generative models will be crucial to meet
        the simulation demands of upcoming surveys.
        For more details, check out my 
        <a href="mos_poster.html">interactive slides</a> from a recent
        conference.
        </p>
        <!--
        <p>
        <h2 id="stats"><u>Probabilistic Galaxy Catalogs</u></h2></br>
        One application for deep generative models and SBI is in constructing
        probabilistic galaxy catalogs. 
        As part of the DESI Galaxy Quasar Physics working group, I'm currently leading the 
        construction of a <u>fully probabilistic value-added catalog of BGS galaxies (PROVABGS)</u> 
        from jointly fitting photometry and spectra. 
        This catalog will provide full posteriors of the physical properties of
        BGS galaxies and enable more sophisticated and rigorous statistical
        analyses such as proper population inference and hierarchical
        probabilistic methods.  
        With 10+million galaxies, PROVABGS will provide the low redshift anchor
        in understanding galaxy evolution for upcoming high redshift surveys
        (e.g. JWST and Roman). 
        </p>
        <p>
        A full Bayesian spectral energy distribution analysis for a single
        galaxy takes ~100 CPU hours. 
        To jointly analyze the photometry and spectra of 10+million BGS
        galaxies without using a billion CPU hours, I am developing new methods
        that address the challenges of massively scaling up current analyses. 
        In [<a href="https://arxiv.org/abs/1911.11778">1911.11778</a>], we
        built neural emulators for SED models that speed up likelihood
        evaluations by >1000x. 
        This still will not be sufficient for future experiments like Roman,
        which will survey a <i>billion</i> galaxies. 
        I am currently developing a normalizing flow-based SBI framework that
        will accelerate the statistical analysis even further. 
        The code for analyzing PROVABGS is publicly <a href="https://github.com/changhoonhahn/provabgs/">available</a>. 
        </p> 
        -->
    </section>

    <footer> 
        &copy; 2021 ChangHoon Hahn <br> 
        feel free to drop me a line at 
        <a href="mailto:changhoon.hahn@princeton.edu">changhoon.hahn@princeton.edu</a>.
    </footer> 
</div>
</body>
</html>
